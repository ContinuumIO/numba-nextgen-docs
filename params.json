{"name":"Numba-nextgen-docs","tagline":"Documentation for numba-nextgen","body":"Next iteration of Numba - The core language ideas\r\n=================================================\r\n\r\nThis document describes the core numba language, which is designed to\r\ngenerate efficient code for general, pythonic code. It will allow us to\r\nimplement most of the features that currently reside in the compiler\r\ndirectly in a runtime. On top of this small core language we can write\r\nmore advanced features such as subtype polymorphism through method\r\ntables.\r\n\r\nI believe we need the following features:\r\n\r\n> -   Methods on user-defined types with specified representations\r\n>     (structs or otherwise)\r\n>\r\n>     > -   Careful control over allocation, mutability and ownership\r\n>\r\n> -   Polymorphism: Generic functions, traits, overloading\r\n>\r\n>     > -   subtyping and inheritance is left to a runtime\r\n>     >     implementation\r\n>     > -   dynamic dispatch for traits is left to a runtime implementation\r\n>     >     :   -   static dispatch only requires some type checking\r\n>     >             support\r\n>     >\r\n> -   User-defined typing rules\r\n> -   Careful control over inlining, unrolling and specialization\r\n> -   Array oriented computing: map/reduce/scan/etc\r\n> -   Extension of the code generator\r\n>\r\nSupport for multi-stage programming would be nice, but is considered a\r\nbonus and deferred to external tools like macropy or mython for now. The\r\ncontrol over optimizations likely provides enough functionality to\r\ngenerate good code.\r\n\r\nThis describes a closed environment with an optionally static, inferred,\r\nlanguage. Static typing will help provide better error messages, and can\r\nprevent inintended use.\r\n\r\nPolymorphism is provided through:\r\n\r\n> -   generic (monomorphized) functions (like C++ templates)\r\n> -   overloading\r\n> -   traits (like interfaces)\r\n> -   subtyping (\"python classes\")\r\n\r\nThis language's goals are ultimate control over performance, and a\r\nlanguage with a well-defined and easily understood subset for the GPU.\r\n\r\nThis language is inspired by the following languages: Rust, Terra,\r\nRPython, Julia, Parakeet, mypy, copperhead. The traits are very similar\r\nto Rust's traits, and are related to type classes in Haskell and\r\ninterfaces in Go.\r\n\r\nHowever, Go interfaces do not allow type-based specialization, and hence\r\nneed runtime type tagging and method dispatch through vtables. Type\r\nconversion between interfaces needs to be runtime-checked type (and new\r\nvtables build at those points, if not cached). Compile-time overloading\r\nis precluded. In Go, interfaces specify what something *can do*, as\r\nopposed to what something *can be*. This can be a useful in a few\r\nsituations, but it means we cannot constrain what things can be (e.g.\r\nany numeric type).\r\n\r\nIn julia we can constrain the types we operate over, which happens\r\nthrough subtyping. E.g.:\r\n\r\n```julia\r\njulia> Int <: Integer\r\ntrue\r\njulia> Int <: Real\r\ntrue\r\njulia> Int <: FloatingPoint\r\nfalse\r\n```\r\n\r\nSo we can define a function which operates only over numbers:\r\n\r\n    julia> function f(x :: Number)\r\n             return x * x\r\n           end\r\n\r\nHere's a the generated code when `x` is an `Int`:\r\n\r\n```llvm\r\njulia> disassemble(f, (Int,))\r\n\r\ndefine %jl_value_t* @f618(%jl_value_t*, %jl_value_t**, i32) {\r\ntop:\r\n  %3 = load %jl_value_t** %1, align 8, !dbg !5256\r\n  %4 = getelementptr inbounds %jl_value_t* %3, i64 0, i32 0, !dbg !5256\r\n  %5 = getelementptr %jl_value_t** %4, i64 1, !dbg !5256\r\n  %6 = bitcast %jl_value_t** %5 to i64*, !dbg !5256\r\n  %7 = load i64* %6, align 8, !dbg !5256\r\n  %8 = mul i64 %7, %7, !dbg !5263\r\n  %9 = call %jl_value_t* @jl_box_int64(i64 %8), !dbg !5263\r\n  ret %jl_value_t* %9, !dbg !5263\r\n}\r\n```\r\n\r\nDisassembling with `Number` generates a much larger chunk of code, which\r\nuses boxed code and ultimately (runtime) multiple dispatch of the `*`\r\nfunction:\r\n\r\n```llvm\r\n%15 = call %jl_value_t* @jl_apply_generic(%jl_value_t* inttoptr (i64 4316726176 to %jl_value_t*), %jl_value_t** %.sub, i32 2), !dbg !5191\r\n```\r\n\r\nHowever, since the implementation of a function is specialized for the\r\nsupertype, it doesn't know the concrete subtype. Type inference can help\r\nprevent these situations and use subtype-specialized code. However, it's\r\nvery easy to make it generate slow code:\r\n\r\n```julia\r\njulia> function g(c)\r\n     if c > 2\r\n       x = 2\r\n     else\r\n       x = 3.0\r\n     end\r\n     return f(x)\r\n   end\r\n\r\njulia> disassemble(g, (Bool,))\r\n```\r\n\r\nThis prints a large chunk of LLVM code (using boxed values), since we\r\nare unifying an Int with a Float. Using both ints, or both floats\r\nhowever leads to very efficient code.\r\n\r\nWhat we want in our language is full control over specialization and\r\nmemory allocation, and easily-understood semantics for what works on the\r\nGPU and what doesn't. The following sections will detail how the above\r\nfeatures will get us there.\r\n\r\n1. User-defined Types\r\n=====================\r\n\r\nWe want to support user-defined types with:\r\n\r\n> -   control over representation\r\n> -   (special) methods\r\n> -   control over mutability\r\n> -   control over stack- vs gc-allocation\r\n\r\nUser-defined types do not support inheritance, which is left to a\r\nruntime implementation. This means that the callees of call-sites are\r\nstatic, and can be called directly. This further means they can be\r\ninlined (something we will exploit).\r\n\r\nThis means that we can even write the most performance-critical parts of\r\nour runtime in this way. The compiler needs to support the following\r\ntypes natively:\r\n\r\n> -   int\r\n> -   float\r\n> -   pointer\r\n> -   struct (with optional methods and properties)\r\n> -   union\r\n> -   array (constant size)\r\n\r\nAnything else is written in the runtime:\r\n\r\n> -   range\r\n> -   complex\r\n> -   array\r\n> -   string/unicode\r\n> -   etc\r\n\r\nThis means we can easily experiment with different data representations\r\nand extend functionality. For instance we can wrap and override the\r\nnative integer multiply to check for overflow, and raise an exception or\r\nissue a warning, or convert to a BigInt.\r\n\r\nRepresentation\r\n--------------\r\n\r\nType representation can be specified through a type 'layout':\r\n\r\n```python\r\n@jit\r\nclass Array(object):\r\n    layout = Struct([('data', 'Char *')])\r\n```\r\n\r\nMutability and Allocation\r\n-------------------------\r\n\r\nEach individual field can be specified to be immutable, or all can be\r\nspecified immutable through a decorator:\r\n\r\n```python\r\n@jit(immutable=True)\r\nclass Array(object):\r\n    ...\r\n```\r\n\r\nIf all fields are immutable, the object can be stack allocated. Unless\r\nmanually specified with `stack=True`, the compiler is free to decide\r\nwhere to allocate the object. This decision may differ depending on the\r\ntarget (cpu or gpu).\r\n\r\nThe `Array` above can be stack-allocated since its fields are immutable\r\n-even though the contained data may not be.\r\n\r\nIf data is mutable, it is allocated on the heap. This means that\r\nallocation of such an object is incompatible with a GPU code generator.\r\nHence, data structures like Arrays must be passed in from the host, and\r\nthings like Lists are not supported. However, one can write a List\r\nimplementation with static size that supports appending a bounded number\r\nof objects.\r\n\r\nWe disallow explicit stack allocation for mutable types for the\r\nfollowing reason:\r\n\r\n```python\r\nx = mutable() # stack allocate\r\ny = x         # copy x into y\r\ny.value = 1   # update y.value, which does not affect x.value\r\n```\r\n\r\nTo make this work one would need to track the lifetimes of the object\r\nitself and all the variables the object is written into, at which point\r\nwe defer you to the Rust programming language. We leave stack allocation\r\nof mutable objects purely as a compile-time optimization.\r\n\r\nDestructors\r\n-----------\r\n\r\nDestructors are supported only for heap-allocated types, irrespective of\r\nmutability. If a \\_\\_del\\_\\_ method is implemented, the object will be\r\nautomatically heap-allocated (unless escape analysis can say otherwise).\r\n\r\nOwnership\r\n---------\r\n\r\nOwnership is tied to mutability:\r\n\r\n> -   Data is owned when (recursively) immutable\r\n> -   Data is shared when it, or some field is mutable (recursively)\r\n\r\nOwned data may be send over a channel to another thread or task. Shared\r\ndata cannot be send, unless explicitly marked as a safe operation:\r\n\r\n    channel.send(borrow(x))\r\n\r\nThe user must guarantee that 'x' stays alive while it is consumed. This\r\nis useful for things like parallel computation on arrays.\r\n\r\nType Parameters\r\n---------------\r\n\r\nUser-defined types are parameterizable:\r\n\r\n```python\r\n@jit('Array[Type dtype, Int ndim]')\r\nclass Array(object):\r\n    ...\r\n```\r\n\r\nParameters can be types or values of builtin type int. This allows\r\nspecialization for values, such as the dimensionality of an array:\r\n\r\n```python\r\n@jit('Array[Type dtype, Int ndim]')\r\nclass Array(object):\r\n\r\n    layout = Struct([('data', 'Char *'), ('strides', 'Tuple[Int, ndim]')])\r\n\r\n    @signature('Tuple[Int, ndim] -> T')\r\n    def __getitem__(self, indices):\r\n        ...\r\n```\r\n\r\nThis specifies that we take a `Tuple` of `Int`s an size `ndim` as\r\nargument, and return an item of type `T`. The `T` and `ndim` are\r\nresolved as type parameters, which means they specify concrete types in\r\nthe method signature.\r\n\r\nThe type can now be used as follows:\r\n\r\n```python\r\nmyarray = Array[Double, 2]()\r\n```\r\n\r\nThis will mostly appear in (numba) library code, and not in user-written\r\ncode, which uses higher-level APIs that ultimately construct these\r\ntypes. E.g.:\r\n\r\n```python\r\n@overload(np.ndarray)\r\ndef typeof(array):\r\n    return Array[typeof(array.dtype), array.ndim]\r\n\r\n@overload(np.dtype)\r\ndef typeof(array):\r\n    return { np.double: Double, ...}[array.dtype]\r\n```\r\n\r\n2. Polymorphism\r\n===============\r\n\r\nSupported forms of polymorphism are generic functions, overloading,\r\ntraits and subtyping and inheritance.\r\n\r\nGeneric Functions (@autojit)\r\n----------------------------\r\n\r\nGeneric functions are like `@autojit`, they provide specialized code for\r\neach unique combination of input types. They may be optionally typed and\r\nconstrained (through traits).\r\n\r\n```python\r\n@jit('(a -> b) -> [a] -> [b]')\r\ndef map(f, xs):\r\n    ...\r\n```\r\n\r\nThis specifies a map implementation that is specialized for each\r\ncombination of type instances for type variables \\`a\\` and \\`b\\`. Type\r\nvariables may be further constrained through traits, in a similar way to\r\nRust's traits\r\n([http://static.rust-lang.org/doc/tutorial.html\\#traits)](http://static.rust-lang.org/doc/tutorial.html#traits)),\r\nallowing you to operate for instance only on arrays of numbers, or\r\narrays of floating point values.\r\n\r\nTraits\r\n------\r\n\r\nTraits specify an interface that value instances implement. Similarly to\r\nRust's traits and Haskell's type classes, they are a form of bounded\r\npolymorphism, allowing users to constrain type variables (\"this function\r\noperates on floating point values only\").\r\n\r\nThey also specify a generic interface that objects can implement.\r\nClasses can declare they belong to a certain trait, allowing any\r\ninstance of the class to be used through the trait:\r\n\r\n```python\r\n@jit('(a -> b) -> Iterable[a] -> [b]')\r\ndef map(f, xs):\r\n    ...\r\n```\r\n\r\nOur map now takes an iterable and returns a list. Written this way, a\r\nsingle map implementation now works for *any* iterable. Any value\r\nimplementing the Iterable trait can now be used:\r\n\r\n```python\r\n@jit('Array[Type dtype, Int ndim]')\r\nclass Array(Iterable['dtype']):\r\n    ...\r\n```\r\n\r\nWe can now use map() over our array. The generated code must now insert\r\na \\`conversion\\` between `Array[dtype, ndim]` and trait\r\n`Iterable[dtype]`, which concretely means packing up a vtable pointer\r\nand a boxed Array pointer. This form of polymorphism will likely be\r\n*incompatible with the GPU backend*. However, we can still use our\r\ngeneric functions by telling the compiler to specialize on input types:\r\n\r\n```python\r\n@specialize.argtypes('f', 'xs')\r\n@jit('(a -> b) -> Iterable[a] -> [b]')\r\ndef map(f, xs):\r\n    ...\r\n```\r\n\r\nAlternatively, we can allow them to simply constrain type variables, and\r\nnot actually specify the type as the trait. The type is supplied instead\r\nby the calling context:\r\n\r\n```python\r\n@signature('(it:Iterable[a]) => (a -> b) -> it -> [b]')\r\ndef map(f, xs):\r\n    ...\r\n```\r\n\r\nThe constraints are specified in similar way to Haskell's type classes.\r\nThe only implementation required in the compiler to support this is the\r\ntype checking feature, otherwise it's entirely the same as generic\r\nfunctions above. Multiple constraints can be expressed, e.g.\r\n`(it:Iterable[a], a:Integral)`. Alternative syntax could be '(a -\\> b)\r\n-\\> lst : Iterable[a] -\\> [b]', but this is less clear when 'it' is\r\nreused elsewhere as a type variable.\r\n\r\nTraits can further use inheritance and have default implementations.\r\nThis can be trivially implemented at the Python level, requiring no\r\nspecial knowledge in the compiler.\r\n\r\nOverloading and Multiple-dispatch\r\n---------------------------------\r\n\r\nThese mechanisms provide compile-time selection for our language. It is\r\nrequired to support the compiled `convert` from section 3, and necessary\r\nfor many implementations, e.g.:\r\n\r\n```python\r\n@jit('Int -> Int')\r\ndef int(x):\r\n    return x\r\n\r\n@jit('String -> Int')\r\ndef int(x):\r\n    return parse_int(x)\r\n```\r\n\r\nOverloading is also provided for methods:\r\n\r\n```python\r\n@jit\r\nclass SomeNeatClass(object):\r\n    @signature('Int -> Int')\r\n    def __add__(self, other):\r\n        return self.value + other\r\n\r\n    @signature('String -> Int')\r\n    def __add__(self, other):\r\n        return str(self.value) + other\r\n```\r\n\r\nWe further need a way to \"overload\" python functions to provide a way to\r\nprovide alternative implementations or to type it. We can easily provide\r\nimplementations for all builtins:\r\n\r\n```python\r\npytypedef(builtins.int, int)\r\n```\r\n\r\n3. User-defined Typing Rules\r\n============================\r\n\r\nI think Julia does really well here. Analogously we define three\r\nfunctions:\r\n\r\n> -   typeof(pyobj) -\\> Type\r\n> -   convert(Type, Value) -\\> Value\r\n> -   unify(Type, Type) -\\> Type\r\n\r\nThe `convert` function may make sense as a method on the objects\r\ninstead, which is more pythonic, e.g. `__convert__`. `unify` does not\r\nreally make sense as a method since it belongs to neither of the two\r\narguments.\r\n\r\nUnify takes two types and returns the result type of the given types.\r\nThis result type can be specified by the user. For instance, we may\r\ndetermine that `unify(Int, Float)` is `Union(Int, Float)`, or that it is\r\n`Float`. The union will give the same result as Python would, but it is\r\nalso more expensive in the terms of the operations used on it (and\r\npotentially storage capacity). Unify is used on types only at control\r\nflow merge points.\r\n\r\nA final missing piece are a form of ad-hoc polymophism, namely\r\ncoercions. This is tricky in the presence of overloading, where multiple\r\ncoercions are possible, but only a single coercion is preferable. E.g.:\r\n\r\n```python\r\n@overload('Float32 -> Float32 -> Float32')\r\ndef add(a, b):\r\n    return a + b\r\n\r\n@overload('Complex64 -> Complex64 -> Complex64')\r\ndef add(a, b):\r\n    return a + b\r\n```\r\n\r\nWhich implementation is `add(1, 2)` supposed to pick, `Int` freely\r\ncoerces to both `Float32` and `Complex64`? Since we don't want built-in\r\ncoercion rules, which are not user-overridable or extensible, we need\r\nsome sort of coercion function. We choose a function\r\n`coercion_distance(src_type, dst_type)` which returns the supposed\r\ndistance between two types, or raises a TypeError. Since this is not\r\ncompiled, we decide to not make it a method of the source type.\r\n\r\n```python\r\n@overload(Int, Float)\r\ndef coercion_distance(int_type, float_type):\r\n    return ...\r\n```\r\n\r\nThese functions are used at compile time to determine which conversions\r\nto insert, or whether to issue typing errors.\r\n\r\n4. Optimization and Specialization\r\n==================================\r\n\r\nWe need to allow careful control over optimizations and code\r\nspecialization. This allows us to use the abstractions we need, without\r\npaying them if we know we can't afford it. We propose the following\r\nintrinsics exposed to users:\r\n\r\n> -   `for x in unroll(iterable): ...`\r\n> -   `@specialize.arg(0)`\r\n\r\nUnrolling\r\n---------\r\n\r\nThe first compiler intrinsic allows unrolling over constant iterables.\r\nFor instance, the following would be a valid usage:\r\n\r\n```python\r\nx = (1, 2, 3)\r\nfor i in unroll(x):\r\n    ...\r\n```\r\n\r\nAn initial implementation will likely simply recognize special container\r\ntypes (Tuple, List, etc). Later we may allow arbitrary (user-written!)\r\niterables, where the result of `len()` must be ultimately constant\r\n(after inlining and register promotion).\r\n\r\nSpecialization\r\n--------------\r\n\r\nThe ability to specialize on various things, similar to specialization\r\nin rpython (`rpython/rlib/objectmodel.py`).\r\n\r\nThese decorators should also be supported as extra arguments to\r\n`@signature` etc.\r\n\r\n5. Data-parallel Operators\r\n==========================\r\n\r\nParakeet and copperhead do this really well. We need map, reduce, zip,\r\nlist comprehensions, etc.\r\n\r\n6. Extension of the Code Generator\r\n==================================\r\n\r\nWe can support an `@opaque` decorator that marks a function or method as\r\n\"opaque\", which means it must be resolved by the code generator. A\r\ndecorator `@codegen(thefunc)` registers a code generator function for\r\nthe function or method being called:\r\n\r\n```python\r\n@jit('Int[Int size]')\r\nclass Int(object):\r\n    @opague('Int -> Int', eval_if_const=True)\r\n    def __add__(self, other):\r\n        return a + b\r\n\r\n@codegen(Int.__add__)\r\ndef emit_add(codegen, self, other):\r\n    # 'self' and 'other' are (typed) pykit values\r\n    return codegen.builder.add(self, other)\r\n```\r\n\r\nThis can also be useful to retain high-level information, instead of\r\nexpanding it out beforehand. This can enable high-level optimizations,\r\ne.g. consider the following code:\r\n\r\n```python\r\nL = []\r\nfor i in range(n):\r\n    L.append(i)\r\n\r\nL = map(f, L)\r\n```\r\n\r\nIf we expand `L = []` and `L.append(i)` into memory allocations and\r\nresizes before considering the `map`, we forgo a potential optimization\r\nwhere the compiler performs loop fusion and eliminates the intermediate\r\nlist.\r\n\r\nSo an opague function *may* have an implementation, but it may be\r\nresolved at a later stage during the pipeline if it is still needed:\r\n\r\n```python\r\n@codegen(List.__init__)\r\ndef emit_new_list(codegen, self):\r\n    return codegen.builder.new_list(self.type)\r\n\r\n@llcodegen('new_list')\r\ndef emit_new_list(codegen, self):\r\n    return codegen.gen_call(List.__init__)\r\n```\r\n\r\nThis should be done with low-level code that doesn't need further\r\nhigh-level optimizations. Users must also ensure this process terminates\r\n(there must be no cycles the call graph).\r\n\r\nConclusion\r\n==========\r\n\r\nThe mechanisms above allow us to easily evaluate how code will be\r\ncompiled, and asses the performance implications. Furthermore, we can\r\neasily see what is GPU incompatible, i.e. anything that:\r\n\r\n> -   uses CFFI (this implies use of Object, which is implemented in\r\n>     terms of CFFI)\r\n> -   uses traits that don't merely constrain type variables\r\n> -   allocates anything mutable\r\n\r\nEverything else should still work.\r\nFusion\r\n======\r\n\r\nWe want to fuse operations producing intermediate structures such as\r\nlists or arrays. Fusion or deforestation has been attempted in various\r\nways, we will first cover some of the existing research in the field.\r\n\r\nDeforestation\r\n-------------\r\n\r\n### build/foldr\r\n\r\nRewrite rules can be used to specify patterns to perform fusion ([1]\\_,\r\n[2]\\_, [3]\\_), e.g.:\r\n\r\n    map f (map g xs) = map (f . g) xs\r\n\r\nThe dot represents the composition operator. To avoid the need for a\r\npattern for each pair of operators, we can express fusable higher-order\r\nfunctions in terms of a small set of combinators. One approach is\r\nbuild/foldr, where `build` generates a list, and `foldr` (reduce)\r\nconsumes it ([3]). Foldr can be defined as follows:\r\n\r\n```haskell\r\nfoldr f z []     = z\r\nfoldr f z (x:xs) = f x (foldr f z xs)\r\n```\r\n\r\n`build` is the dual of `foldr`, instead of reducing a list it generates\r\none. Using just build and foldr, a single rewrite rule can be used for\r\ndeforestation:\r\n\r\n> foldr k z (build g) = g k z\r\n\r\nThis is easy to understand considering that build generates a list, and\r\nfoldr then consumes it, so there's no point in building it in the first\r\nplace. Build is specified as follows:\r\n\r\n```haskell\r\nbuild g (:) []\r\n```\r\n\r\nThis means `g` is applied to the `cons` constructor and the empty list.\r\nWe can define a range function (`from` in [3]) as follows:\r\n\r\n```haskell\r\nrange a b = if a > b then []\r\n            else a : (range (a + 1) b)\r\n```\r\n\r\nAbstracting over cons and nil (the empty list) [3], we get:\r\n\r\n```haskell\r\nrange' a b = \\ f lst -> if a > b then lst\r\n                        else f a (range' (a + 1) b f lst)\r\n```\r\n\r\nIt's easy to see the equivalence to `range` above by substituting `(:)`\r\nfor `f` and `[]` for lst. We can now use `range'` with `build` ([3]):\r\n\r\n```haskell\r\nrange a b = build (range' a b)\r\n```\r\n\r\nThings like `map` can now be expressed as follows ([3]):\r\n\r\n```haskell\r\nmap f xs = build (\\ cons lst -> foldr (\\ a b -> cons (f a) b) lst xs)\r\n```\r\n\r\nHowever, some functions cannot be expressed in this framework, like zip\r\n([4]\\_).\r\n\r\n### Streams\r\n\r\nAnother major approach is based on stream fusion ([4]\\_, [5]\\_). It\r\nexpresses the higher-order functions in terms of streams ([4]\\_):\r\n\r\n```haskell\r\nmap f = unstream . map' f . stream\r\n```\r\n\r\n`unstream` converts a stream back to a list, and stream converts a list\r\nto a stream. Under composition, like `map f (map g xs)`, we get\r\n`unsteam . map' f . stream . unsteam . map' g . stream`. The fusion then\r\nrelies on eliminating the composition of `stream` with `unstream`:\r\n\r\n> stream (unstream s) = s\r\n\r\nA stream consists of a stepper function and a state. Stepper functions\r\nproduce new step states. The states are `Done`, `Yield` or `Skip`.\r\n`Done` signals that the stream is consumed, `Yield` yields a new value\r\nand state, and `Skip` signals that a certain value needs to be skipped\r\n(for things like filter).\r\n\r\nLet's see this in action ([5]):\r\n\r\n```haskell\r\nstream :: [a] -> Stream a\r\nstream xs0 = Stream next xs0\r\n    where\r\n        next []     = Done\r\n        next (x:xs) = Yield x xs\r\n```\r\n\r\nThis converts a list to a Stream. It constructs a Stream with a new\r\nstepper function `next` and the initial state (the given list). The\r\n`next` stepper function produces a new step state every time it is\r\ncalled. Streams can be consumed as follows:\r\n\r\n```haskell\r\nmap f (Stream next0 s0) = Stream next s0\r\n    where\r\n        next s = case next0 s of\r\n            Done        -> Done\r\n            Skip s'     -> Skip s'\r\n            Yield x s'  -> Yield (f x) s'\r\n```\r\n\r\nHere we specify a new stepper function `next` that, given a state,\r\nadvances the stream it consumes with the new state, and yields new\r\nresults. It wraps this stepper function in a new stream. [5]\\_ further\r\nextends this work to allow operation over various kinds of streams:\r\n\r\n> -   Chunked streams for bulk memory operations\r\n> -   Vector (multi) streams for SIMD computation\r\n> -   Normal streams that yield one value at a time\r\n\r\nIt bundles the various streams together in a product type. The idea is\r\nthat all streams are available at the same time. Hence a producer can\r\nproduce in the most efficient way, and the consumer can consume in the\r\nmost efficient way. These concepts don't always align, in which case\r\nfallbacks are in place, for instance a chunked stream can be processed\r\nas a scalar stream, or vice-versa. In addition to inlining and other\r\noptimizations it relies heavily on call-pattern specialization ([6]),\r\nallowing the compiler to eliminate pattern matching of consumer sites.\r\n\r\nFusion in Numba\r\n---------------\r\n\r\nThe concept of a stream encapsulating a state and a stepper function is\r\nakin to iterators in Python, where the state is part of the iterator and\r\nthe stepping functionality is provided by the `__next__` method.\r\nAlthough iterators can be composed and specialized on static callee\r\ndestination ( the \\_\\_next\\_\\_ method of another iterator), they are\r\nmost naturally expressed as generators:\r\n\r\n    def map(f, xs):\r\n        for x in xs:\r\n            yield f(xs)\r\n\r\nThe state is naturally captured in the generator's stack frame. To allow\r\nfusion we need to inline producers into consumers. This is possible only\r\nif we can turn the lazy generator into a non-lazy producer, i.e. the\r\nconsumer must immediately consume the result. This introduces a\r\nrestriction:\r\n\r\n> -   The generator may not be stored, passed to other functions or\r\n>     returned. We can capture this notion by having `iter(generator)`\r\n>     create a `stream`, and disallowing the rewrite rule\r\n>     `stream (unstream s) = s` to trigger when the `unstream` has\r\n>     multiple uses.\r\n>\r\n>     This means the value remains \\`unstreamed\\` (which itself is lazy,\r\n>     but effectively constitutes a fusion boundary).\r\n>\r\nSince we can express many (all?) higher-order fusable functions as\r\ngenerator, we have a powerful building block (in the same way as the\r\npreviously outlined research methods), that will give us rewrite rules\r\nfor free. I.e., we will not need to state the following:\r\n\r\n```python\r\nmap(f, map(g, xs)) = map(f . g, xs)\r\n```\r\n\r\nsince this automatically follows from the definition of map:\r\n\r\n```python\r\n@signature('(a -> b) -> Stream a -> Stream b')\r\ndef map(f, xs):\r\n    for x in xs:\r\n        yield f(x)\r\n```\r\n\r\nThe two things that need to be addressed are 1) how to inline generators\r\nand 2) how do we specialize on argument \"sub-terms\".\r\n\r\n1. Inlining Generators\r\n----------------------\r\n\r\nThe inlining pattern is straightforward:\r\n\r\n> -   remove the loop back-edge\r\n> -   promote loop index to stack variable\r\n> -   inline generator\r\n> -   transform 'yield val' to 'i = val'\r\n> -   replace each 'yield' from the callee with a copy of the loop body\r\n>     of the caller\r\n\r\nNow consider a set of generators that have multiple yield expressions:\r\n\r\n```python\r\ndef f(x):\r\n    yield x\r\n    yield x\r\n```\r\n\r\nInlining of the producer into the consumer means duplicating the body\r\nfor each yield. This can lead to exponential code explosion in the size\r\nof the depth of the terms:\r\n\r\n```python\r\nfor i in f(f(f(x))):\r\n    print i\r\n```\r\n\r\nWill result in a function with 8 print statements. However, it is not\r\nalways possible to generate static code without multiple yields,\r\nconsider the concatenation function:\r\n\r\n```python\r\ndef concat(xs, ys):\r\n    for x in xs:\r\n        yield x\r\n    for y in ys:\r\n        yield ys\r\n```\r\n\r\nThis function has two yields. If we rewrite it to use only one yield:\r\n\r\n```python\r\ndef concat(xs, ys):\r\n    for g in (xs, ys):\r\n        for x in g:\r\n            yield x\r\n```\r\n\r\nWe have introduced dynamicity that cannot be eliminated without\r\nspecialization on the values (i.e. unrolling the outer loop, yielding\r\nthe first implementation). This not special in any way, it is inherent\r\nto inlining and we and treat it as such (by simply using an inlining\r\nthreshold). Crossing the threshold simply means temporaries are not\r\neliminated -- in this case this means generator \"cells\" remain.\r\n\r\nIf this proves problematic, functions such as concat can instead always\r\nunstream their results. Even better than fully unstreaming, or sticking\r\nwith a generator cell, is to use a buffering generator fused with the\r\nexpression that consumes N iterations and buffers the results. This\r\ndivides the constant overhead of generators by a constant factor.\r\n\r\n### 2. Specialization\r\n\r\nSpecialization follows from inlining, there are two cases:\r\n\r\n> -   internal terms\r\n> -   boundary terms\r\n> -   `stream (unstream s)` is rewritten, the result is fused\r\n\r\nInternal terms are rewritten according to the `stream (unstream s)`\r\nrule. What eventually follows at a boundary is a) consumption through a\r\nuser-written loop or b) consumption through the remaining unstream. In\r\neither case the result is consumed, and the inliner will start inlining\r\ntop-down (reducing the terms top-down).\r\n\r\nSIMD Producers\r\n--------------\r\n\r\nFor simplicity we exclude support for chunked streams. Analogous to\r\n[5]\\_ we can expose a SIMD vector type to the user. This vector can be\r\nyielded by a producer to a consumer.\r\n\r\nHow then, does a consumer pick which stream to operate on? For instance,\r\nzip can only efficiently be implemented if both inputs are the same, not\r\nif one returns vectors and the other scalars (or worse, switching back\r\nand forth mid-way):\r\n\r\n```python\r\ndef zip(xs, ys):\r\n    while True:\r\n        try:\r\n            yield (next(xs), next(ys))\r\n        except StopIteration:\r\n            break\r\n```\r\n\r\nFor functions like zip, which are polymorphic in their arguments, we can\r\nsimply constrain our inputs:\r\n\r\n```python\r\n@overload('Stream[Vector a] -> Stream[Vector b] -> Stream[(Vector a, Vector b)]')\r\n@overload('Stream a -> Stream b -> Stream (a, b)')\r\ndef zip(xs, ys):\r\n    ...\r\n```\r\n\r\nOf course, this means if one of the arguments produces vectors, and the\r\nother scalars, we need to convert one to the other:\r\n\r\n```python\r\n@overload('Stream[Vector a] -> Stream a')\r\ndef convert(stream):\r\n    for x in stream:\r\n        yield x\r\n```\r\n\r\nWhich basically unpacks values from the SIMD register.\r\n\r\nAlternatively, a mixed stream of vectors and scalars can be consumed.\r\n[5]\\_ distinguises between two vector streams:\r\n\r\n> -   a producer stream, which can yield Vector | Scalar\r\n> -   a consumer stream, where the consumer chooses whether to read\r\n>     vectors or scalars. A consumer can start with vectors, and when\r\n>     the vector stream is consumed read from the scalar stream.\r\n\r\nA producer stream is useful for producers that mostly yield vectors, but\r\nsometimes need to yield a few scalars. This class includes functions\r\nlike concat that concatenates two streams, or e.g. a stream over a\r\nmulti-dimensional array where inner-contiguous dimensions have a number\r\nof elements not 0 modulo the vector size.\r\n\r\nA consumer stream on the other hand is useful for functions like zip,\r\nallowing them to vectorize part of the input. However, this does not\r\nseem terribly useful for multi-dimensional arrays with contiguous rows,\r\nwhere it would only vectorize the first row and then fall back to\r\nscalarized code.\r\n\r\nHowever, neither model really makes sense for us, since we would already\r\nmanually specialize our loops:\r\n\r\n```python\r\n@overload('Array a 2 -> Stream a')\r\ndef stream_array(array, vector_size):\r\n    for row in array:\r\n        for i in range(len(row) / vector_size):\r\n            yield load_vector(row.data + i * 4)\r\n\r\n        for i in range(i * 4, len(row)):\r\n            yield row[i]\r\n```\r\n\r\nThis means code consuming scalars and code consuming vectors can be\r\nmatched up through pattern specialiation (which is not just type-based\r\nbranch pruning).\r\n\r\nTo keep things simple, we will stick with a producer stream, yielding\r\neither vectors or scalars. Consumers then pattern-match on the produced\r\nvalues, and pattern specialization can then switch between the two\r\nalternatives:\r\n\r\n```python\r\ndef sum(xs):\r\n    vzero = Vector(zero)\r\n    zero = 0\r\n    for x in xs:\r\n        if isinstance(x, Vector):\r\n            vzero += x\r\n        else:\r\n            zero += x\r\n    return zero + vreduce(add, vzero)\r\n```\r\n\r\nTo understand pattern specialization, consider `xs` is a\r\n`stream_array(a)`. This results in approximately the following code\r\nafter inlining:\r\n\r\n```python\r\nstream_array(array, vector_size):\r\n    for row in array:\r\n        for i in range(len(row) / vector_size):\r\n            x = load_vector(row.data + i * 4)\r\n            if isinstance(x, Vector):\r\n                vzero += x\r\n            else:\r\n                zero += x\r\n\r\n        for i in range(i * 4, len(row)):\r\n            x = row[i]\r\n            if isinstance(x, Vector):\r\n                vzero += x\r\n            else:\r\n                zero += x\r\n```\r\n\r\nIt is now easy to see that we can eliminate the second pattern in the\r\nfirst loop, and the first pattern in the second loop.\r\n\r\nCompiler Support\r\n----------------\r\n\r\nTo summarize, to support fusion in a general and pythonic way can be\r\nmodelled on generators. To support this we need:\r\n\r\n> -   generator inlining\r\n> -   For SIMD and bulk operations, call pattern specialization. For us\r\n>     this means branch pruning and branch merging based on type.\r\n\r\nThe most important optimization is the fusion, SIMD is a useful\r\nextension. Depending on the LLVM vectorizer (or possibly our own), it\r\nmay not be necessary.\r\n\r\nReferences\r\n==========\r\nTyping\r\n======\r\n\r\nThis section discusses typing for numba. There is plenty of literature\r\non type inference, most notable is the Damas-Hindley-Milner Algorithm W.\r\nfor lambda calculus [1]\\_, and an extension for ML. The algorithm\r\nhandles let-polymorphism (a.k.a. ML-polymorphism), a form of parametric\r\npolymorphism where type variables themselves may not be polymorphic. For\r\nexample, consider:\r\n\r\n```python\r\ndef f(g, x):\r\n    g(x)\r\n    g(0)\r\n```\r\n\r\nWe can call `f` with a function, which must accept `x` and an value of\r\ntype int. Since `g` is a monotype in `f`, the second call to `g`\r\nrestricts what we accept for `x`: it must be something that promotes\r\nwith an integer. In other words, the type for `g` is `a -> b` and not\r\n`âˆ€a,b.a -> b`.\r\n\r\nAlthough linear in practise, the algorithm's worst case behaviour is\r\nexponential ([2]\\_), since it does not share results for different\r\nfunction invocations. The cartesian product algorithm ([3]\\_) avoids\r\nthis by sharing monomorphic template instantiations. It considers all\r\npossible receivers of a message send, and takes the union of the results\r\nof all instances of the cartesian product substitution. The paper does\r\nnot seem to address circular type dependencies, where the receiver can\r\nchange based on the input types:\r\n\r\n```python\r\ndef f(x):\r\n    for i in range(10):\r\n        x = g(x)\r\n```\r\n\r\nleading to\r\n\r\n```llvm\r\ndefine void f(X0 %x0) {\r\ncond:\r\n    %0 = lt %i 10\r\n    cbranch %0 body exit\r\n\r\nbody:\r\n    %x1 = phi(x0, x2)\r\n    %x2 = call g(%x0)\r\n    br cond\r\n\r\nexit:\r\n    ret void\r\n}\r\n```\r\n\r\nHowever, this can be readily solved through fix-point iteration. If we\r\nassign type variables throughout the function first, we get the\r\nfollowing constraints:\r\n\r\n    [ X1 = Union(X0, X2), G = X1 -> T2 , X2 = T2 ]\r\n\r\nWe can represent a function as a set of overloaded signatures. However,\r\nthe function application is problematic, since we send X1 (which will be\r\nassigned a union type). WIthout using the cartesian product this would\r\nlead to exponential behaviour since there are 2\\^N subsets for N types.\r\n\r\nType inference in Numba\r\n=======================\r\n\r\nWe use the cartesian product algorithm on a constraint network based on\r\nthe dataflow graph. To understand it, we need to understand the input\r\nlanguage. Since we put most functionality of the language in the\r\nuser-domain, we desugar operator syntax through special methods, and we\r\nfurther support overloaded functions.\r\n\r\nThe front-end generates a simple language that can conceptually be\r\ndescribed through the syntax below:\r\n\r\n    e = x                           variable\r\n      | x = a                       assignment\r\n      | const(x)                    constants\r\n      | x.a                         attribute\r\n      | f(x)                        application\r\n      | jump/ret/exc_throw/...      control flow\r\n      | (T) x                       conversion\r\n\r\nAs you'll notice, there are no operators, loops, etc. Control flow is\r\nencoded through jumps, exception raising, return, etc. Loops can be\r\nreadily detected through a simple analysis (see\r\npykit/analysis/loop\\_detection.py).\r\n\r\nWe take this input grammar and generate a simpler constraint network,\r\nthat looks somewhat like this:\r\n\r\n    e = x.a             attribute\r\n      | f(x)            application\r\n      | flow(a, b)      data flow\r\n\r\nThis is a directed graph where each node classifies the constraint on\r\nthe inputs. Types propagate through this network until no more changes\r\ncan take place. If there is an edge `A -> B`, then whenever `A` is\r\nupdated, types are propagated to `B` and processed according to the\r\nconstraint on `B`. E.g. if `B` is a function call, and `A` is an input\r\nargument, we analyze the function call with the new values in the\r\ncartesian product.\r\n\r\nCoercions\r\n=========\r\n\r\nCoercions may happen in two syntactic constructs:\r\n\r\n> -   application\r\n> -   control flow merges (phi nodes)\r\n\r\nFor application we have a working implementation in Blaze that\r\ndetermines the best match for polymorphic type signatures, and allows\r\nfor coercions. For control flow merges, the user can choose whether to\r\npromote values, or whether to create a sum-type. A post-pass can simply\r\ninsert coercions where argument types do not match parameter types.\r\n\r\nSubtyping\r\n=========\r\n\r\nWe intend to support subtyping in the runtime through python\r\ninheritance. When a class B inherits from a class A, we check for a\r\ncompatible interface for the methods (argument types are contravariant\r\nand return types covariant). When typing, the only thing we need to\r\nimplement are coercion and unification:\r\n\r\n> Type B coerces to type A if B is a subtype of A Type A coerces to type\r\n> B if B is a subtype of A with a runtime check only\r\n\r\nThen class types A and B unify iff A is a subtype of B or vice-versa.\r\nThe result of unification is always the supertype.\r\n\r\nFinally, parameteric types will be classified invariant, to avoid\r\nunintended mistakes in the face of mutable containers. Consider e.g.\r\nsuperclass `A` and subclass `B`. Assume we have the function that\r\naccepts an argument typed `A[:]`. If we treat the dtype as covariant,\r\nthen we may pass an array `B[:]` for that argument. However, the code\r\ncan legally write `A`s into the array, violating the rule that we can\r\nonly assign subtypes. The problem is that reading values is covariant,\r\nwhereas writing is contravariant. In other words, the parameter must be\r\ncovariant as well as contravariant at the same time, which is only\r\nsatisfied when `A = B`.\r\n\r\nThe exception is maybe function types, for which we have built-in\r\nvariance rules.\r\n\r\nParameterization\r\n================\r\n\r\nTypes can only be parameterized by variables and user-defined or\r\nbuilt-in types. Type variables may be constrained through traits (type\r\nsets can readily be constructed by implementing (empty) traits).\r\n\r\nReferences\r\n==========\r\nNumba Runtime\r\n=============\r\n\r\nNearly all built-in data types are implemented in the runtime.\r\n\r\nGarbage Collector\r\n=================\r\n\r\nTo support mutable heap-allocated types, we need a garbage collector. To\r\nget started quickly we can use Boehm or reference counting. We will want\r\nto port one of the available copying collectors and use a shadowstack or\r\na lazy pointer stack (for bonus points). The GC should then be local to\r\neach thread, since there is no shared state between threads (only owned\r\nand borrowed data is allowed).\r\n\r\nGarbage collection is abstracted by pykit.\r\n\r\nExceptions\r\n==========\r\n\r\nExceptions are also handled by pykit. We can implement several models,\r\ndepending on the target architecture:\r\n\r\n> -   costful (error return codes)\r\n>     :   -   This will be used on the GPU\r\n>\r\n> -   zero-cost\r\n>     :   -   This should be used where supported. We will start with\r\n>             costful\r\n>\r\n> -   setjmp/longjmp\r\n>     :   -   This will need to happen for every stack frame in case of\r\n>             a shadow stack\r\n>\r\nLocal exception handling will be translated to jumps. This is not\r\ncontrived, since we intend to make heavy use of inlining:\r\n\r\n```python\r\nwhile 1:\r\n    try:\r\n        i = x.__next__()\r\n    except StopIteration:\r\n        break\r\n```\r\n\r\n`x.__next__()` may be inlined (and will be in many instances, like\r\nrange()), and the `raise StopIteration` will be translated to a jump.\r\nControl flow simplification can further optimize the extra jump (jump to\r\nbreak, break to loop exit).\r\n\r\nThreads\r\n=======\r\n\r\nAs mentioned in the core language overview, memory is not shared unless\r\nborrowed. This process is unsafe and correctness must be ensured by the\r\nuser. Immutable data can be copied over channels between threads. Due to\r\na thread-local GC, all threads can run at the same time and allocate\r\nmemory at the same time.\r\n\r\nWe will remove prange and simply use a parallel map with a closure.\r\n\r\nTraits\r\n======\r\n\r\nTraits are mostly a compile-time type-checking detail and some simple\r\nruntime decorator support. Traits with dynamic dispatch require vtables,\r\nsomething we can implement in the runtime as well:\r\n\r\n> [https://github.com/zdevito/terra/blob/master/tests/lib/golike.t](https://github.com/zdevito/terra/blob/master/tests/lib/golike.t)\r\n\r\nExtension Types\r\n===============\r\n\r\nExtension types are currently built on top of CPython objects. This\r\nshould be avoided. We need to decouple numba with anything CPython, for\r\nthe sake of portability as well as pycc.\r\n\r\nExtension types can also easily be written in the runtime:\r\n\r\n> -   `unify()` needs to return the supertype or raise a type error\r\n> -   `convert(obj, Object)` needs to do a runtime typecheck\r\n> -   `coerce_distance` needs to return a distance for how far the\r\n>     supertype is up the inheritance tree\r\n\r\nThe approach is simple: generate a wrapper method for each method in the\r\nextension type that does a vtable lookup.\r\n\r\nClosures\r\n========\r\n\r\nThis time we will start with the most common case: closures consumed as\r\ninner functions. This means we don't need dynamic binding for our cell\r\nvariables, and we can do simple lambda lifting instead of complicated\r\nclosure conversion. This also trivially works on the GPU, allowing one\r\nto use map, filter etc, with lambdas trivially.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}